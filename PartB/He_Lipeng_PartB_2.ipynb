{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-widedeep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "num_features = [\n",
    "    \"dist_to_nearest_stn\",\n",
    "    \"dist_to_dhoby\",\n",
    "    \"degree_centrality\",\n",
    "    \"eigenvector_centrality\",\n",
    "    \"remaining_lease_years\",\n",
    "    \"floor_area_sqm\",\n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    \"month\",\n",
    "    \"town\",\n",
    "    \"flat_model_type\",\n",
    "    \"storey_range\",\n",
    "]\n",
    "\n",
    "features = num_features + cat_features\n",
    "\n",
    "targets = [\"resale_price\"]\n",
    "\n",
    "df = pd.read_csv(\"hdb_price_prediction.csv\")\n",
    "\n",
    "df_train = df[df[\"year\"] <= 2020]\n",
    "df_test = df[df[\"year\"] >= 2021]\n",
    "\n",
    "train = df_train[features + targets]\n",
    "test = df_test[features + targets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonyh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:10<00:00, 129.80it/s, loss=2.3e+5] \n",
      "epoch 2: 100%|██████████| 1366/1366 [00:09<00:00, 136.83it/s, loss=9.89e+4]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:09<00:00, 138.64it/s, loss=8.62e+4]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:09<00:00, 142.02it/s, loss=7.93e+4]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:09<00:00, 142.73it/s, loss=7.55e+4]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:09<00:00, 142.57it/s, loss=7.29e+4]\n",
      "epoch 7: 100%|██████████| 1366/1366 [00:09<00:00, 141.89it/s, loss=7.14e+4]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:10<00:00, 129.35it/s, loss=6.99e+4]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:10<00:00, 129.79it/s, loss=6.9e+4] \n",
      "epoch 10: 100%|██████████| 1366/1366 [00:10<00:00, 136.32it/s, loss=6.82e+4]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:09<00:00, 140.90it/s, loss=6.75e+4]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:11<00:00, 121.07it/s, loss=6.71e+4]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:09<00:00, 142.84it/s, loss=6.64e+4]\n",
      "epoch 14: 100%|██████████| 1366/1366 [00:09<00:00, 147.50it/s, loss=6.61e+4]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:10<00:00, 126.36it/s, loss=6.59e+4]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:10<00:00, 132.40it/s, loss=6.56e+4]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:10<00:00, 132.55it/s, loss=6.53e+4]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:10<00:00, 129.85it/s, loss=6.49e+4]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:08<00:00, 152.80it/s, loss=6.49e+4]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:08<00:00, 156.11it/s, loss=6.49e+4]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:08<00:00, 154.18it/s, loss=6.44e+4]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:09<00:00, 151.56it/s, loss=6.44e+4]\n",
      "epoch 23: 100%|██████████| 1366/1366 [00:08<00:00, 152.00it/s, loss=6.38e+4]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:08<00:00, 156.21it/s, loss=6.38e+4]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:09<00:00, 143.64it/s, loss=6.37e+4]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:09<00:00, 148.09it/s, loss=6.36e+4]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:08<00:00, 155.62it/s, loss=6.36e+4]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:08<00:00, 156.45it/s, loss=6.33e+4]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:08<00:00, 152.45it/s, loss=6.3e+4] \n",
      "epoch 30: 100%|██████████| 1366/1366 [00:08<00:00, 163.15it/s, loss=6.28e+4]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:08<00:00, 163.23it/s, loss=6.28e+4]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:08<00:00, 162.62it/s, loss=6.26e+4]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:08<00:00, 164.98it/s, loss=6.26e+4]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:08<00:00, 165.88it/s, loss=6.22e+4]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:08<00:00, 156.68it/s, loss=6.21e+4]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:08<00:00, 161.17it/s, loss=6.21e+4]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:08<00:00, 164.69it/s, loss=6.17e+4]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:08<00:00, 163.86it/s, loss=6.17e+4]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:08<00:00, 163.17it/s, loss=6.15e+4]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:08<00:00, 163.24it/s, loss=6.15e+4]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:08<00:00, 164.59it/s, loss=6.11e+4]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:08<00:00, 160.68it/s, loss=6.12e+4]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:08<00:00, 159.09it/s, loss=6.09e+4]\n",
      "epoch 44: 100%|██████████| 1366/1366 [00:08<00:00, 163.00it/s, loss=6.1e+4] \n",
      "epoch 45: 100%|██████████| 1366/1366 [00:10<00:00, 130.66it/s, loss=6.09e+4]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:10<00:00, 127.78it/s, loss=6.07e+4]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:10<00:00, 134.41it/s, loss=6.04e+4]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:09<00:00, 138.85it/s, loss=6.04e+4]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:10<00:00, 134.75it/s, loss=6.01e+4]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:10<00:00, 135.85it/s, loss=6.01e+4]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:09<00:00, 138.83it/s, loss=5.99e+4]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:09<00:00, 137.78it/s, loss=5.98e+4]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:09<00:00, 137.52it/s, loss=5.96e+4]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:09<00:00, 138.50it/s, loss=5.95e+4]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:09<00:00, 138.34it/s, loss=5.94e+4]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:10<00:00, 136.01it/s, loss=5.94e+4]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:10<00:00, 131.55it/s, loss=5.91e+4]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:09<00:00, 137.10it/s, loss=5.92e+4]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:09<00:00, 136.70it/s, loss=5.89e+4]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:09<00:00, 138.29it/s, loss=5.9e+4] \n",
      "epoch 61: 100%|██████████| 1366/1366 [00:09<00:00, 138.19it/s, loss=5.88e+4]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:10<00:00, 136.22it/s, loss=5.87e+4]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:09<00:00, 138.23it/s, loss=5.84e+4]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:10<00:00, 135.83it/s, loss=5.82e+4]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:09<00:00, 137.12it/s, loss=5.83e+4]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:09<00:00, 139.32it/s, loss=5.83e+4]\n",
      "epoch 67: 100%|██████████| 1366/1366 [00:09<00:00, 136.82it/s, loss=5.77e+4]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:10<00:00, 125.55it/s, loss=5.74e+4]\n",
      "epoch 69: 100%|██████████| 1366/1366 [00:10<00:00, 136.45it/s, loss=5.7e+4] \n",
      "epoch 70: 100%|██████████| 1366/1366 [00:09<00:00, 146.91it/s, loss=5.65e+4]\n",
      "epoch 71: 100%|██████████| 1366/1366 [00:08<00:00, 161.56it/s, loss=5.55e+4]\n",
      "epoch 72: 100%|██████████| 1366/1366 [00:09<00:00, 138.74it/s, loss=5.49e+4]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:09<00:00, 149.78it/s, loss=5.4e+4] \n",
      "epoch 74: 100%|██████████| 1366/1366 [00:09<00:00, 142.89it/s, loss=5.33e+4]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:10<00:00, 131.00it/s, loss=5.28e+4]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:08<00:00, 167.60it/s, loss=5.24e+4]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:10<00:00, 134.54it/s, loss=5.23e+4]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:09<00:00, 149.58it/s, loss=5.19e+4]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:08<00:00, 166.14it/s, loss=5.18e+4]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:08<00:00, 157.40it/s, loss=5.15e+4]\n",
      "epoch 81: 100%|██████████| 1366/1366 [00:09<00:00, 136.88it/s, loss=5.15e+4]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:09<00:00, 144.84it/s, loss=5.15e+4]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:08<00:00, 156.28it/s, loss=5.12e+4]\n",
      "epoch 84: 100%|██████████| 1366/1366 [00:08<00:00, 159.88it/s, loss=5.12e+4]\n",
      "epoch 85: 100%|██████████| 1366/1366 [00:08<00:00, 156.81it/s, loss=5.11e+4]\n",
      "epoch 86: 100%|██████████| 1366/1366 [00:11<00:00, 121.00it/s, loss=5.1e+4] \n",
      "epoch 87: 100%|██████████| 1366/1366 [00:10<00:00, 130.40it/s, loss=5.08e+4]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:10<00:00, 127.72it/s, loss=5.1e+4] \n",
      "epoch 89: 100%|██████████| 1366/1366 [00:10<00:00, 133.54it/s, loss=5.07e+4]\n",
      "epoch 90: 100%|██████████| 1366/1366 [00:10<00:00, 133.56it/s, loss=5.07e+4]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:10<00:00, 133.61it/s, loss=5.06e+4]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:10<00:00, 131.31it/s, loss=5.04e+4]\n",
      "epoch 93: 100%|██████████| 1366/1366 [00:10<00:00, 133.28it/s, loss=5.03e+4]\n",
      "epoch 94: 100%|██████████| 1366/1366 [00:10<00:00, 132.27it/s, loss=5.03e+4]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:10<00:00, 129.42it/s, loss=5.03e+4]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:10<00:00, 130.78it/s, loss=5.02e+4]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:10<00:00, 132.09it/s, loss=5e+4]   \n",
      "epoch 98: 100%|██████████| 1366/1366 [00:10<00:00, 132.72it/s, loss=5e+4]   \n",
      "epoch 99: 100%|██████████| 1366/1366 [00:10<00:00, 133.03it/s, loss=4.99e+4]\n",
      "epoch 100: 100%|██████████| 1366/1366 [00:10<00:00, 133.58it/s, loss=4.99e+4]\n"
     ]
    }
   ],
   "source": [
    "tab_preprocessor = TabPreprocessor(\n",
    "    embed_cols=cat_features, continuous_cols=num_features\n",
    ")\n",
    "X_tab = tab_preprocessor.fit_transform(train)\n",
    "\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=num_features,\n",
    "    mlp_hidden_dims=[200, 100],\n",
    ")\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    objective=\"root_mean_squared_error\",\n",
    "    num_workers=0,\n",
    "    seed=SEED,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "trainer.fit(\n",
    "    X_tab=X_tab, target=train[\"resale_price\"].values, n_epochs=100, batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:03<00:00, 359.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 97072.19440824342\n",
      "Test R2: 0.6707784730545674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r2_score = R2Score()\n",
    "\n",
    "X_tab_te = tab_preprocessor.transform(test)\n",
    "y_pred = trainer.predict(X_tab=X_tab_te)\n",
    "\n",
    "y_test = np.array(test[\"resale_price\"])\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "rmse = np.sqrt(np.sum((y_test - y_pred) ** 2) / len(y_test))\n",
    "r2 = r2_score(y_pred, y_test)\n",
    "\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test R2: {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
